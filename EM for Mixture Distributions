## Mixture Distribution Problem
In this problem, we have a mixture of two binomial random variables with mixture parameter $q$. These binomials share a size, $m$, but they have success probabilities $p_1$ and $p_2$, respectively. The EM algorithm maximizes the posterior log likelihood given the previous iteration.

### Distributional Problem Statement
We first state the problem by specifying the distribution from which we obtain our sample. After stating the distribution of the sample, we can maximize the incomplete log likelihood via an iterative method. 

In addition to the mixture distribution as stated in the problem, we will assign latent variables $Z_1, Z_2, \cdots, Z_n$ that have the property that if $Z_i=1$ then we know $X_i\sim bin(m, p_1)$. Thus, by the statement of our problem and the definition of our latent variables, we have:
$$
X_1, X_2, X_2, \cdots, X_n\sim qbin(m,p_1)+(1-q)bin(m,p_2)\\
X_i|Z_i=1\sim bin(m, p_1)
$$

We will first use this to find the posterior distribution of $Z_i|X_i$ using Bayes' rule.


$$
P(Z_i=1|X_i)=\frac{P(X_i|Z_i=1)P(Z_i=1)}{P(X_i)}
            =\frac{{m\choose x_i}p_1^{x_i}(1-p_1)^{m-x_i}\cdot q}{{m\choose x_i}p_1^{x_i}(1-p_1)^{m-x_i}\cdot q+{m\choose x_i}p_2^{x_i}(1-p_2)^{m-x_i}\cdot (1-q)}
            =\gamma_{1i}
$$

We will write this formula to obtain the $r^{th}$ iteration of $\gamma_{1i}$, denoted $\hat{\gamma}_{1i}^{(r)}$, in the following way
$$
\hat{\gamma}_{1i}^{(r)}=\frac{{m\choose x_i}(\hat{p}_1^{(r-1)})^{x_i}(1-\hat{p}_1^{(r-1)})^{m-x_i}\cdot \hat{q}^{(r-1)}}{{m\choose x_i}(\hat{p}_1^{(r-1)})^{x_i}(1-\hat{p}_1^{(r-1)})^{m-x_i}\cdot \hat{q}^{(r-1)}+{m\choose x_i}(\hat{p}_2^{(r-1)})^{x_i}(1-\hat{p}_2^{(r-1)})^{m-x_i}\cdot (1-\hat{q}^{(r-1)})}
$$
Now, to perform the EM, we will find the expected likelihood of our sample with respect to the latent $z_i$'s. To simplify notation, we let $Q(\Theta^{(r)}, \Theta^{(r-1)})=E_{Z|X, p_1, p_2, q}[L(X|p_1, p_2, q, Z)]$ and $q(\Theta^{(r)}, \Theta^{(r-1)})=E_{Z|X, p_1, p_2, q}[log(L(X|p_1, p_2, q, Z))]$ where $\Theta=(q, p_1, p_2)$.

$$
Q(\Theta^{(r)}, \Theta^{(r-1)})=E_{Z|X, p_1, p_2, q}[\Pi_{i=1}^n[{m\choose x_i} p_1^{x_i} (1-p_1)^{m-x_1} q]^{z_i} \cdot[{m\choose x_i} p_2^{x_i} (1-p_2)^{m-x_1} (1-q)]^{1-z_i}]
$$
We now will find the expected log-likelihood, which is given by

$$
q(\Theta^{(r)}, \Theta^{(r-1)})=E_{Z|X, p_1, p_2, q}[log(\Pi_{i=1}^n[{m\choose x_i} p_1^{x_i} (1-p_1)^{m-x_1} q]^{z_i} \cdot[{m\choose x_i} p_2^{x_i} (1-p_2)^{m-x_1} (1-q)]^{1-z_i}])\\
                =E_{Z|X, p_1, p_2, q}[\Sigma_{i=1}^n[z_i[log{m\choose x_i}+x_i log(p_1)+(m-x_i)log(1-p_1)+log(q)]+(1-z_i)[log{m\choose x_i}+x_i log(p_2)+(m-x_i)log(1-p_2)+log(1-q)]]]
$$

Now that we have the expected log-likelihood, we can push through our expectation to obtain
$$
q(\Theta^{(r)}, \Theta^{(r-1)})=\Sigma_{i=1}^n[\hat{\gamma}_{1i}^{(r)}[log{m\choose x_i}+x_i log(p_1)+(m-x_i)log(1-p_1)+log(q)]+(1-\hat{\gamma}_{1i}^{(r)})[log{m\choose x_i}+x_i log(p_2)+(m-x_i)log(1-p_2)+log(1-q)]]
$$
We will now take the derivative with respect to the $(r+1)^{th}$ iterations, and since $\hat{\gamma}_{1i}^{(r)}$ is based on the $(r-1)^{th}$ iteration, it will be treated as a constant. We now differentiate with respect to $q$ and set this equal to $0$ first:

$$
\dfrac{dq(\Theta^{(r)}, \Theta^{(r-1)})}{dq}=\Sigma_{i=1}^n[\hat{\gamma}_{1i}^{(r)}\frac{1}{q}-(1-\hat{\gamma}_{1i}^{(r)})\frac{1}{1-q}]=0\\
\frac{1}{q(1-q)}\Sigma_{i=1}^n[\hat{\gamma}_{1i}^{(r)}(1-q)-(1-\hat{\gamma}_{1i}^{(r)})q]=0\\
\frac{1}{q(1-q)}\Sigma_{i=1}^n[\hat{\gamma}_{1i}^{(r)}-q]=0\\
\Sigma_{i=1}^n\hat{\gamma}_{1i}^{(r)}=nq\\
\frac{\Sigma_{i=1}^n\hat{\gamma}_{1i}^{(r)}}{n}=\hat{q}^{(r+1)}
$$


Next, we will take the derivative with respect to $p_1$, which gives
$$
\dfrac{dq(\Theta^{(r)}, \Theta^{(r-1)})}{dq}=\Sigma_{i=1}^n[\hat{\gamma}_{1i}^{(r)}[\frac{x_i}{p_1} -\frac{m-x_i}{1-p_1}]]=0\\
\frac{1}{p_1(1-p_1)}\Sigma_{i=1}^n[\hat{\gamma}_{1i}^{(r)}[x_i(1-p_1) -(m-x_i)p_1]]=0\\
\Sigma_{i=1}^n[\hat{\gamma}_{1i}^{(r)}[x_i -mp_1]]=0\\
\Sigma_{i=1}^n\hat{\gamma}_{1i}^{(r)}x_i= \Sigma_{i=1}^n\hat{\gamma}_{1i}^{(r)}mp_1\\
\frac{\Sigma_{i=1}^n\hat{\gamma}_{1i}^{(r)}x_i}{m\Sigma_{i=1}^n\hat{\gamma}_{1i}^{(r)}}=\hat{p_1}^{(r+1)}
$$

From the symmetry of the problem, we find that
$$
\frac{\Sigma_{i=1}^n(1-\hat{\gamma}_{1i}^{(r)})x_i}{m\Sigma_{i=1}^n(1-\hat{\gamma}_{1i}^{(r)})}=\hat{p_2}^{(r+1)}
$$
Now we have an iterative method:

* Initialize values for $\hat{q}, \hat{p_1}, \hat{p_2}$
* Use the values to find $\hat{\gamma}_{1i}$
* Use $\hat{\gamma}_{1i}$ to update $\hat{q}$, $\hat{p_1}$, $\hat{p_2}$
* Repeat steps 2-3 until convergence

Lastly, all we must do is implement this code in R. The R code is in this repository and covers data generation and running the algorithm.
